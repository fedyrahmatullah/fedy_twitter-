{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use if not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alhamdulilah orang jalan pt ibrahim daftar vak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antri ambulance bawa pasien covid dijogja mlm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>april banser tanggap bencana bagana bagana kab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as dukung indonesia lawan tingkat covid sindon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as kirim juta vaksin covid moderna indonesia j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ppkm darurat laku juli laksana tekan sebar cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ppkm darurat laku patuh tekan laju tular covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ppkm darurat sesuai instruksi presiden jokowi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ppkm langkah sejarah ivermectin obat cacing du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>proning the miracle of sujud sembuh serang covid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet\n",
       "0   alhamdulilah orang jalan pt ibrahim daftar vak...\n",
       "1   antri ambulance bawa pasien covid dijogja mlm ...\n",
       "2   april banser tanggap bencana bagana bagana kab...\n",
       "3   as dukung indonesia lawan tingkat covid sindon...\n",
       "4   as kirim juta vaksin covid moderna indonesia j...\n",
       "..                                                ...\n",
       "57  ppkm darurat laku juli laksana tekan sebar cov...\n",
       "58  ppkm darurat laku patuh tekan laju tular covid...\n",
       "59  ppkm darurat sesuai instruksi presiden jokowi ...\n",
       "60  ppkm langkah sejarah ivermectin obat cacing du...\n",
       "61   proning the miracle of sujud sembuh serang covid\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "#df = pd.read_csv(\"Tweet_output_fedy-nostopword.csv\")\n",
    "df = pd.read_excel(\"positif_train_xl.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alhamdulilah orang jalan pt ibrahim daftar vak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antri ambulance bawa pasien covid dijogja mlm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>april banser tanggap bencana bagana bagana kab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as dukung indonesia lawan tingkat covid sindon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as kirim juta vaksin covid moderna indonesia j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ppkm darurat laku juli laksana tekan sebar cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ppkm darurat laku patuh tekan laju tular covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ppkm darurat sesuai instruksi presiden jokowi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ppkm langkah sejarah ivermectin obat cacing du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>proning the miracle of sujud sembuh serang covid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet\n",
       "0   alhamdulilah orang jalan pt ibrahim daftar vak...\n",
       "1   antri ambulance bawa pasien covid dijogja mlm ...\n",
       "2   april banser tanggap bencana bagana bagana kab...\n",
       "3   as dukung indonesia lawan tingkat covid sindon...\n",
       "4   as kirim juta vaksin covid moderna indonesia j...\n",
       "..                                                ...\n",
       "57  ppkm darurat laku juli laksana tekan sebar cov...\n",
       "58  ppkm darurat laku patuh tekan laju tular covid...\n",
       "59  ppkm darurat sesuai instruksi presiden jokowi ...\n",
       "60  ppkm langkah sejarah ivermectin obat cacing du...\n",
       "61   proning the miracle of sujud sembuh serang covid\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stopword\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_indonesia = stopwords.words('indonesian')\n",
    " \n",
    "#import sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    #remove coma\n",
    "    tweet = re.sub(r',','',tweet)\n",
    "    \n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_tokennew'] = df['Tweet'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ambil data Tweet(bersih) - make code as a comment if don't use\n",
    "df.drop(df.columns[[0]], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Token_train1.csv',encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_tokennew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['alhamdulilah', 'orang', 'jalan', 'pt', 'ibra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['antri', 'ambulance', 'bawa', 'pasien', 'covi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['april', 'banser', 'tanggap', 'bencana', 'bag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['as', 'dukung', 'indonesia', 'lawan', 'tingka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['as', 'kirim', 'juta', 'vaksin', 'covid', 'mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>['ppkm', 'darurat', 'laku', 'juli', 'laksana',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>['ppkm', 'darurat', 'laku', 'patuh', 'tekan', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>['ppkm', 'darurat', 'sesuai', 'instruksi', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>['ppkm', 'langkah', 'sejarah', 'ivermectin', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>['proning', 'the', 'miracle', 'of', 'sujud', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_tokennew\n",
       "0   ['alhamdulilah', 'orang', 'jalan', 'pt', 'ibra...\n",
       "1   ['antri', 'ambulance', 'bawa', 'pasien', 'covi...\n",
       "2   ['april', 'banser', 'tanggap', 'bencana', 'bag...\n",
       "3   ['as', 'dukung', 'indonesia', 'lawan', 'tingka...\n",
       "4   ['as', 'kirim', 'juta', 'vaksin', 'covid', 'mo...\n",
       "..                                                ...\n",
       "57  ['ppkm', 'darurat', 'laku', 'juli', 'laksana',...\n",
       "58  ['ppkm', 'darurat', 'laku', 'patuh', 'tekan', ...\n",
       "59  ['ppkm', 'darurat', 'sesuai', 'instruksi', 'pr...\n",
       "60  ['ppkm', 'langkah', 'sejarah', 'ivermectin', '...\n",
       "61  ['proning', 'the', 'miracle', 'of', 'sujud', '...\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEET_DATA = pd.read_csv('Token_train1.csv')\n",
    "#TWEET_DATA = df\n",
    "TWEET_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antri', 'ambulance', 'bawa', 'pasien', 'covid', 'dijogja', 'mlm', 'moga', 'layan', 'pulih', 'semangat', 'kru', 'ambulance', 'juang', 'hantar', 'pasien', 'semangat', 'nakes', 'rs', 'jogjakarta']\n",
      "\n",
      "type :  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# convert list formated string to list\n",
    "import ast\n",
    "\n",
    "def convert_text_list(texts):\n",
    "    texts = ast.literal_eval(texts)\n",
    "    return [text for text in texts]\n",
    "\n",
    "TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet_tokennew\"].apply(convert_text_list)\n",
    "\n",
    "\n",
    "print(TWEET_DATA[\"tweet_list\"][1])\n",
    "\n",
    "print(\"\\ntype : \", type(TWEET_DATA[\"tweet_list\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       tweet_tokennew  \\\n",
      "0   ['alhamdulilah', 'orang', 'jalan', 'pt', 'ibra...   \n",
      "1   ['antri', 'ambulance', 'bawa', 'pasien', 'covi...   \n",
      "2   ['april', 'banser', 'tanggap', 'bencana', 'bag...   \n",
      "3   ['as', 'dukung', 'indonesia', 'lawan', 'tingka...   \n",
      "4   ['as', 'kirim', 'juta', 'vaksin', 'covid', 'mo...   \n",
      "..                                                ...   \n",
      "57  ['ppkm', 'darurat', 'laku', 'juli', 'laksana',...   \n",
      "58  ['ppkm', 'darurat', 'laku', 'patuh', 'tekan', ...   \n",
      "59  ['ppkm', 'darurat', 'sesuai', 'instruksi', 'pr...   \n",
      "60  ['ppkm', 'langkah', 'sejarah', 'ivermectin', '...   \n",
      "61  ['proning', 'the', 'miracle', 'of', 'sujud', '...   \n",
      "\n",
      "                                           tweet_list  \\\n",
      "0   [alhamdulilah, orang, jalan, pt, ibrahim, daft...   \n",
      "1   [antri, ambulance, bawa, pasien, covid, dijogj...   \n",
      "2   [april, banser, tanggap, bencana, bagana, baga...   \n",
      "3   [as, dukung, indonesia, lawan, tingkat, covid,...   \n",
      "4   [as, kirim, juta, vaksin, covid, moderna, indo...   \n",
      "..                                                ...   \n",
      "57  [ppkm, darurat, laku, juli, laksana, tekan, se...   \n",
      "58  [ppkm, darurat, laku, patuh, tekan, laju, tula...   \n",
      "59  [ppkm, darurat, sesuai, instruksi, presiden, j...   \n",
      "60  [ppkm, langkah, sejarah, ivermectin, obat, cac...   \n",
      "61  [proning, the, miracle, of, sujud, sembuh, ser...   \n",
      "\n",
      "                                              TF_dict  \n",
      "0   {'alhamdulilah': 0.043478260869565216, 'orang'...  \n",
      "1   {'antri': 0.05, 'ambulance': 0.1, 'bawa': 0.05...  \n",
      "2   {'april': 0.0625, 'banser': 0.0625, 'tanggap':...  \n",
      "3   {'as': 0.125, 'dukung': 0.125, 'indonesia': 0....  \n",
      "4   {'as': 0.1111111111111111, 'kirim': 0.05555555...  \n",
      "..                                                ...  \n",
      "57  {'ppkm': 0.07692307692307693, 'darurat': 0.076...  \n",
      "58  {'ppkm': 0.1111111111111111, 'darurat': 0.1111...  \n",
      "59  {'ppkm': 0.08333333333333333, 'darurat': 0.083...  \n",
      "60  {'ppkm': 0.1111111111111111, 'langkah': 0.1111...  \n",
      "61  {'proning': 0.125, 'the': 0.125, 'miracle': 0....  \n",
      "\n",
      "[62 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def calc_TF(document):\n",
    "    # Counts the number of times the word appears in review\n",
    "    TF_dict = {}\n",
    "    for term in document:\n",
    "        if term in TF_dict:\n",
    "            TF_dict[term] += 1\n",
    "        else:\n",
    "            TF_dict[term] = 1\n",
    "    # Computes tf for each word\n",
    "    for term in TF_dict:\n",
    "        TF_dict[term] = TF_dict[term] / len(document)\n",
    "    return TF_dict\n",
    "\n",
    "TWEET_DATA[\"TF_dict\"] = TWEET_DATA['tweet_list'].apply(calc_TF)\n",
    "\n",
    "TWEET_DATA[\"TF_dict\"].head(10)\n",
    "print(TWEET_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term \t TF\n",
      "\n",
      "               antri \t 0.05\n",
      "           ambulance \t 0.1\n",
      "                bawa \t 0.05\n",
      "              pasien \t 0.1\n",
      "               covid \t 0.05\n",
      "             dijogja \t 0.05\n",
      "                 mlm \t 0.05\n",
      "                moga \t 0.05\n",
      "               layan \t 0.05\n",
      "               pulih \t 0.05\n",
      "            semangat \t 0.1\n",
      "                 kru \t 0.05\n",
      "               juang \t 0.05\n",
      "              hantar \t 0.05\n",
      "               nakes \t 0.05\n",
      "                  rs \t 0.05\n",
      "          jogjakarta \t 0.05\n"
     ]
    }
   ],
   "source": [
    "# Check TF result\n",
    "index = 1\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
    "for key in TWEET_DATA[\"TF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_DF(tfDict):\n",
    "    count_DF = {}\n",
    "    # Run through each document's tf dictionary and increment countDict's (term, doc) pair\n",
    "    for document in tfDict:\n",
    "        for term in document:\n",
    "            if term in count_DF:\n",
    "                count_DF[term] += 1\n",
    "            else:\n",
    "                count_DF[term] = 1\n",
    "    return count_DF\n",
    "\n",
    "DF = calc_DF(TWEET_DATA[\"TF_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alhamdulilah': 1,\n",
       " 'orang': 6,\n",
       " 'jalan': 2,\n",
       " 'pt': 2,\n",
       " 'ibrahim': 1,\n",
       " 'daftar': 2,\n",
       " 'vaksin': 5,\n",
       " 'program': 2,\n",
       " 'tubuh': 2,\n",
       " 'team': 1,\n",
       " 'data': 1,\n",
       " 'sumbang': 2,\n",
       " 'rm': 1,\n",
       " 'sakit': 4,\n",
       " 'covid': 61,\n",
       " 'penghulu': 1,\n",
       " 'mukim': 1,\n",
       " 'pontin': 1,\n",
       " 'jpkk': 1,\n",
       " 'antri': 1,\n",
       " 'ambulance': 1,\n",
       " 'bawa': 3,\n",
       " 'pasien': 1,\n",
       " 'dijogja': 1,\n",
       " 'mlm': 1,\n",
       " 'moga': 9,\n",
       " 'layan': 2,\n",
       " 'pulih': 2,\n",
       " 'semangat': 3,\n",
       " 'kru': 1,\n",
       " 'juang': 3,\n",
       " 'hantar': 1,\n",
       " 'nakes': 2,\n",
       " 'rs': 2,\n",
       " 'jogjakarta': 1,\n",
       " 'april': 1,\n",
       " 'banser': 1,\n",
       " 'tanggap': 2,\n",
       " 'bencana': 1,\n",
       " 'bagana': 1,\n",
       " 'kabupaten': 1,\n",
       " 'kalong': 1,\n",
       " 'makam': 1,\n",
       " 'jenazah': 1,\n",
       " 'takut': 1,\n",
       " 'panggil': 2,\n",
       " 'jiwa': 1,\n",
       " 'cinta': 1,\n",
       " 'negeri': 2,\n",
       " 'as': 2,\n",
       " 'dukung': 5,\n",
       " 'indonesia': 8,\n",
       " 'lawan': 6,\n",
       " 'tingkat': 4,\n",
       " 'sindonews': 1,\n",
       " 'bukanberitabiasa': 1,\n",
       " 'kirim': 1,\n",
       " 'juta': 3,\n",
       " 'moderna': 1,\n",
       " 'jabat': 2,\n",
       " 'bahas': 1,\n",
       " 'rencana': 1,\n",
       " 'bantu': 8,\n",
       " 'upaya': 2,\n",
       " 'respons': 1,\n",
       " 'luas': 2,\n",
       " 'atas': 1,\n",
       " 'lonjak': 1,\n",
       " 'gubernur': 3,\n",
       " 'jawa': 3,\n",
       " 'timur': 2,\n",
       " 'khofifah': 1,\n",
       " 'indar': 1,\n",
       " 'parawansa': 1,\n",
       " 'langsung': 1,\n",
       " 'gerak': 1,\n",
       " 'cepat': 4,\n",
       " 'awal': 1,\n",
       " 'beritabaikhariini': 1,\n",
       " 'kerjasama': 2,\n",
       " 'pagi': 3,\n",
       " 'tim': 2,\n",
       " 'pagipagi': 1,\n",
       " 'berita': 1,\n",
       " 'informatif': 1,\n",
       " 'inspiratif': 1,\n",
       " 'update': 1,\n",
       " 'portal': 1,\n",
       " 'ayo': 2,\n",
       " 'olahraga': 1,\n",
       " 'guys': 1,\n",
       " 'keluar': 1,\n",
       " 'keringat': 1,\n",
       " 'imun': 1,\n",
       " 'pakai': 1,\n",
       " 'masker': 2,\n",
       " 'disiplin': 3,\n",
       " 'protokol': 4,\n",
       " 'sehat': 12,\n",
       " 'cegah': 5,\n",
       " 'sebar': 13,\n",
       " 'virus': 5,\n",
       " 'divhumaspolri': 1,\n",
       " 'poldakaltim': 1,\n",
       " 'polrestasamarinda': 1,\n",
       " 'polsektasungaikunjang': 1,\n",
       " 'ayopakaimasker': 1,\n",
       " 'patuhiprotokolkesehatan': 1,\n",
       " 'cegahviruscovid': 1,\n",
       " 'ayotaatim': 1,\n",
       " 'taatiprotokolkesehatan': 1,\n",
       " 'saat': 1,\n",
       " 'pandemi': 7,\n",
       " 'laku': 4,\n",
       " 'ppkm': 10,\n",
       " 'darurat': 10,\n",
       " 'tni': 1,\n",
       " 'polri': 1,\n",
       " 'tdk': 1,\n",
       " 'bosan': 1,\n",
       " 'nya': 4,\n",
       " 'menghimbau': 1,\n",
       " 'kpd': 2,\n",
       " 'warga': 3,\n",
       " 'agr': 1,\n",
       " 'tertib': 1,\n",
       " 'taat': 1,\n",
       " 'prokes': 3,\n",
       " 'hindar': 3,\n",
       " 'terap': 2,\n",
       " 'm': 4,\n",
       " 'dlm': 1,\n",
       " 'hidup': 1,\n",
       " 'hari': 1,\n",
       " 'jgb': 1,\n",
       " 'korban': 1,\n",
       " 'corona': 1,\n",
       " 'bakti': 1,\n",
       " 'jaga': 3,\n",
       " 'selamat': 3,\n",
       " 'masyarakat': 4,\n",
       " 'mari': 6,\n",
       " 'kontribusi': 1,\n",
       " 'tular': 3,\n",
       " 'patuh': 7,\n",
       " 'vaksinasi': 2,\n",
       " 'semangatpresisipolresgorontalo': 1,\n",
       " 'bangkit': 1,\n",
       " 'umat': 2,\n",
       " 'zakat': 1,\n",
       " 'energi': 1,\n",
       " 'kedermawanan': 1,\n",
       " 'kuat': 1,\n",
       " 'krisis': 1,\n",
       " 'pandemik': 2,\n",
       " 'prof': 1,\n",
       " 'hilman': 1,\n",
       " 'latief': 1,\n",
       " 'a': 3,\n",
       " 'ph': 1,\n",
       " 'd': 1,\n",
       " 'miladlazismu': 1,\n",
       " 'lazismu': 1,\n",
       " 'muhammadiyah': 2,\n",
       " 'memberiuntuknegeri': 1,\n",
       " 'zakatbangkitkanindonesia': 1,\n",
       " 'tangan': 2,\n",
       " 'dprd': 1,\n",
       " 'cleaning': 1,\n",
       " 'service': 1,\n",
       " 'perhati': 3,\n",
       " 'detakkaltim': 1,\n",
       " 'barusan': 1,\n",
       " 'cerita': 1,\n",
       " 'lucu': 1,\n",
       " 'dia': 1,\n",
       " 'vonis': 1,\n",
       " 'fucksin': 1,\n",
       " 'x': 1,\n",
       " 'telah': 1,\n",
       " 'ket': 1,\n",
       " 'fauckin': 1,\n",
       " 'si': 1,\n",
       " 'tarik': 1,\n",
       " 'statement': 1,\n",
       " 'saran': 1,\n",
       " 'istirahat': 1,\n",
       " 'minum': 1,\n",
       " 'air': 2,\n",
       " 'putih': 2,\n",
       " 'banyak': 1,\n",
       " 'arti': 1,\n",
       " 'dagang': 1,\n",
       " 'belia': 1,\n",
       " 'masjid': 1,\n",
       " 'lis': 1,\n",
       " 'arau': 1,\n",
       " 'lancar': 1,\n",
       " 'projek': 1,\n",
       " 'food': 1,\n",
       " 'bank': 1,\n",
       " 'inisiatif': 1,\n",
       " 'ringan': 1,\n",
       " 'beban': 1,\n",
       " 'golong': 1,\n",
       " 'kes': 1,\n",
       " 'bersholawat': 1,\n",
       " 'nariyah': 1,\n",
       " 'berdo': 1,\n",
       " 'bangsa': 3,\n",
       " 'bebas': 1,\n",
       " 'wabah': 2,\n",
       " 'blora': 1,\n",
       " 'bismillah': 1,\n",
       " 'allahumma': 1,\n",
       " 'aamiin': 2,\n",
       " 'ariefrohman': 1,\n",
       " 'do': 1,\n",
       " 'doadaridesaumtukindonesia': 1,\n",
       " 'tppimagetan': 1,\n",
       " 'semprit': 1,\n",
       " 'hormat': 1,\n",
       " 'top': 1,\n",
       " 'doyo': 1,\n",
       " 'dianissa': 1,\n",
       " 'rachman': 1,\n",
       " 'puter': 1,\n",
       " 'kaltim': 1,\n",
       " 'ajak': 1,\n",
       " 'samarinda': 1,\n",
       " 'ikut': 1,\n",
       " 'tribun': 1,\n",
       " 'doa': 3,\n",
       " 'desa': 1,\n",
       " 'untukindonesia': 1,\n",
       " 'doadaridesauntukindonesia': 2,\n",
       " 'dokter': 1,\n",
       " 'tenaga': 3,\n",
       " 'medis': 3,\n",
       " 'hadap': 1,\n",
       " 'isoman': 1,\n",
       " 'baca': 1,\n",
       " 'ajar': 1,\n",
       " 'nabi': 1,\n",
       " 'shollallahu': 1,\n",
       " 'alayhi': 1,\n",
       " 'wa': 1,\n",
       " 'sallam': 1,\n",
       " 'ulang': 1,\n",
       " 'hati': 1,\n",
       " 'tenteram': 1,\n",
       " 'in-syaa': 1,\n",
       " 'allah': 1,\n",
       " 'ppkmdarurat': 1,\n",
       " 'langkah': 3,\n",
       " 'perintah': 5,\n",
       " 'putus': 3,\n",
       " 'rantai': 1,\n",
       " 'sobat': 1,\n",
       " 'bijak': 3,\n",
       " 'bijaksana': 1,\n",
       " 'ppkmdaruratlindungikita': 1,\n",
       " 'fungsi': 2,\n",
       " 'jec': 1,\n",
       " 'sbg': 1,\n",
       " 'rumah': 3,\n",
       " 'lbh': 1,\n",
       " 'efektif': 1,\n",
       " 'effisien': 1,\n",
       " 'sgr': 2,\n",
       " 'operasional': 1,\n",
       " 'colek': 1,\n",
       " 'ganjar': 1,\n",
       " 'pranowo': 1,\n",
       " 'puji': 1,\n",
       " 'warung': 1,\n",
       " 'pesan': 2,\n",
       " 'take': 1,\n",
       " 'away': 1,\n",
       " 'bungkus': 1,\n",
       " 'pulang': 1,\n",
       " 'ganjarpranowo': 1,\n",
       " 'hallo': 1,\n",
       " 'kawan': 1,\n",
       " 'jm': 1,\n",
       " 'aktivitas': 1,\n",
       " 'mana': 1,\n",
       " 'lupa': 1,\n",
       " 'yuk': 3,\n",
       " 'halo': 1,\n",
       " 'sahabat': 1,\n",
       " 'lingkung': 2,\n",
       " 'tekan': 6,\n",
       " 'limbah': 1,\n",
       " 'sumber': 1,\n",
       " 'terang': 1,\n",
       " 'lihat': 1,\n",
       " 'infografis': 1,\n",
       " 'yah': 1,\n",
       " 'kelola': 1,\n",
       " 'sampah': 1,\n",
       " 'ivermectin': 2,\n",
       " 'obat': 2,\n",
       " 'terapi': 1,\n",
       " 'produksi': 1,\n",
       " 'indofarma': 1,\n",
       " 'tlah': 1,\n",
       " 'izin': 1,\n",
       " 'edar': 2,\n",
       " 'bpom': 1,\n",
       " 'moeldoko': 1,\n",
       " 'distribusi': 1,\n",
       " 'ribu': 1,\n",
       " 'kota': 1,\n",
       " 'singkawang': 1,\n",
       " 'moeldokomilikihakhukum': 1,\n",
       " 'jakarta': 1,\n",
       " 'provinsi': 1,\n",
       " 'dki': 1,\n",
       " 'buka': 1,\n",
       " 'peluang': 1,\n",
       " 'profesional': 1,\n",
       " 'kendali': 2,\n",
       " 'dinas': 1,\n",
       " 'janganogeb': 1,\n",
       " 'tenang': 1,\n",
       " 'gegabah': 1,\n",
       " 'susu': 1,\n",
       " 'beruang': 1,\n",
       " 'abis': 1,\n",
       " 'cari': 1,\n",
       " 'tempat': 1,\n",
       " 'c': 2,\n",
       " 'dah': 1,\n",
       " 'langka': 1,\n",
       " 'harga': 1,\n",
       " 'vitamin': 1,\n",
       " 'stay': 1,\n",
       " 'safe': 1,\n",
       " 'kabar': 1,\n",
       " 'teliti': 1,\n",
       " 'baru': 1,\n",
       " 'purifier': 1,\n",
       " 'portable': 1,\n",
       " 'hepa': 1,\n",
       " 'filter': 1,\n",
       " 'turun': 1,\n",
       " 'risiko': 1,\n",
       " 'papar': 2,\n",
       " 'aerosol': 1,\n",
       " 'simulasi': 1,\n",
       " 'indoor': 1,\n",
       " 'kombinasi': 1,\n",
       " 'ruang': 1,\n",
       " 'tutup': 1,\n",
       " 'works': 1,\n",
       " 'keingetan': 1,\n",
       " 'pas': 1,\n",
       " 'abi': 1,\n",
       " 'serta': 1,\n",
       " 'alhamdulillah': 1,\n",
       " 'gasampe': 1,\n",
       " 'kasih': 2,\n",
       " 'awat': 1,\n",
       " 'intensive': 1,\n",
       " 'kenal': 1,\n",
       " 'follow': 1,\n",
       " 'retweet': 1,\n",
       " 'teman': 2,\n",
       " 'kali': 1,\n",
       " 'postingan': 1,\n",
       " 'membutukan': 1,\n",
       " 'informasi': 1,\n",
       " 'supply': 1,\n",
       " 'tweet': 1,\n",
       " 'jual': 1,\n",
       " 'barang': 1,\n",
       " 'jasa': 1,\n",
       " 'butuh': 1,\n",
       " 'ketua': 2,\n",
       " 'pbnu': 1,\n",
       " 'lindung': 1,\n",
       " 'klien': 1,\n",
       " 'ambil': 1,\n",
       " 'a-life': 1,\n",
       " 'legasi': 1,\n",
       " 'pd': 1,\n",
       " 'manfaat': 1,\n",
       " 'free': 1,\n",
       " 'diagnosis': 1,\n",
       " 'cover': 1,\n",
       " 'even': 1,\n",
       " 'for': 1,\n",
       " 'home': 1,\n",
       " 'quarantine': 1,\n",
       " 'komplikasi': 1,\n",
       " 'jommmss': 1,\n",
       " 'dapat': 1,\n",
       " 'hibah': 1,\n",
       " 'keluarga': 1,\n",
       " 'wassap': 1,\n",
       " 'klo': 1,\n",
       " 'uang': 1,\n",
       " 'beli': 1,\n",
       " 'hewan': 1,\n",
       " 'qurban': 1,\n",
       " 'tetangga': 1,\n",
       " 'mebutuhkan': 1,\n",
       " 'krn': 2,\n",
       " 'dampak': 1,\n",
       " 'didahulukam': 1,\n",
       " 'berqurban': 1,\n",
       " 'bersadekah': 1,\n",
       " 'pilih': 1,\n",
       " 'sadekah': 1,\n",
       " 'qurbannya': 1,\n",
       " 'aja': 1,\n",
       " 'selesai': 1,\n",
       " 'arab': 1,\n",
       " 'tiada': 1,\n",
       " 'ibadah': 1,\n",
       " 'haji': 1,\n",
       " 'asrama': 1,\n",
       " 'respect': 1,\n",
       " 'menag': 1,\n",
       " 'sednag': 1,\n",
       " 'ya': 1,\n",
       " 'kunci': 1,\n",
       " 'konsumsi': 1,\n",
       " 'makan': 1,\n",
       " 'gizi': 1,\n",
       " 'imbang': 1,\n",
       " 'agam': 1,\n",
       " 'kebal': 1,\n",
       " 'optimal': 1,\n",
       " 'jenis': 1,\n",
       " 'the': 2,\n",
       " 'british': 1,\n",
       " 'nutrition': 1,\n",
       " 'foundation': 1,\n",
       " 'menteri': 1,\n",
       " 'selaras': 1,\n",
       " 'pick': 1,\n",
       " 'khairy': 1,\n",
       " 'jamaluddin': 1,\n",
       " 'anggar': 1,\n",
       " 'terima': 2,\n",
       " 'libat': 1,\n",
       " 'sukarela': 1,\n",
       " 'astrazeneca': 1,\n",
       " 'janji': 1,\n",
       " 'temu': 1,\n",
       " 'dekat': 1,\n",
       " 'komunikasikita': 1,\n",
       " 'jabatanpenerangan': 1,\n",
       " 'kurang': 2,\n",
       " 'bumi': 1,\n",
       " 'ppkmnya': 1,\n",
       " 'hasil': 1,\n",
       " 'laju': 2,\n",
       " 'varian': 1,\n",
       " 'apa': 1,\n",
       " 'garda': 1,\n",
       " 'depan': 1,\n",
       " 'tanggulang': 2,\n",
       " 'serah': 1,\n",
       " 'terus': 1,\n",
       " 'rakyat': 2,\n",
       " 'ramah': 1,\n",
       " 'sopan': 1,\n",
       " 'santun': 1,\n",
       " 'bukti': 1,\n",
       " 'buru': 2,\n",
       " 'anggap': 1,\n",
       " 'dar': 1,\n",
       " 'goyang': 1,\n",
       " 'ubur-ubur': 1,\n",
       " 'nkri': 1,\n",
       " 'nggak': 1,\n",
       " 'mempan': 1,\n",
       " 'iklim': 1,\n",
       " 'tropis': 1,\n",
       " 'jamu': 1,\n",
       " 'kalung': 1,\n",
       " 'eucalyptus': 1,\n",
       " 'muka': 1,\n",
       " 'agama': 1,\n",
       " 'sempat': 1,\n",
       " 'penuh': 1,\n",
       " 'syarat': 1,\n",
       " 'mending': 1,\n",
       " 'pademi': 1,\n",
       " 'bom': 1,\n",
       " 'cucitanganpakaisabun': 1,\n",
       " 'pakaimaskermu': 1,\n",
       " 'jagajarak': 1,\n",
       " 'cerah': 1,\n",
       " 'riah': 1,\n",
       " 'bagi': 1,\n",
       " 'berkat': 1,\n",
       " 'tuhan': 1,\n",
       " 'manusia': 1,\n",
       " 'cipta': 1,\n",
       " 'pandang': 1,\n",
       " 'beda': 1,\n",
       " 'selisih': 1,\n",
       " 'salam': 1,\n",
       " 'protokoler': 1,\n",
       " 'adat': 1,\n",
       " 'baduy': 1,\n",
       " 'nol': 1,\n",
       " 'persen': 1,\n",
       " 'pentadbiran': 1,\n",
       " 'amerika': 1,\n",
       " 'syarikat': 1,\n",
       " 'sahkan': 1,\n",
       " 'dos': 1,\n",
       " 'pfizer': 1,\n",
       " 'biontech': 1,\n",
       " 'malaysia': 1,\n",
       " 'pegawai': 1,\n",
       " 'isnin': 1,\n",
       " 'julai': 1,\n",
       " 'presiden': 2,\n",
       " 'jo': 1,\n",
       " 'biden': 1,\n",
       " 'komited': 1,\n",
       " 'dunia': 1,\n",
       " 'percaya': 1,\n",
       " 'supir': 1,\n",
       " 'ambul': 1,\n",
       " 'tugas': 1,\n",
       " 'gali': 1,\n",
       " 'kubur': 1,\n",
       " 'guna': 1,\n",
       " 'duduk': 1,\n",
       " 'bla': 1,\n",
       " 'wakanda': 1,\n",
       " 'usaha': 1,\n",
       " 'personil': 1,\n",
       " 'sat': 1,\n",
       " 'lantas': 1,\n",
       " 'laksana': 2,\n",
       " 'yustisi': 1,\n",
       " 'mandiri': 1,\n",
       " 'polda': 1,\n",
       " 'bal': 1,\n",
       " 'polres': 1,\n",
       " 'klungkung': 1,\n",
       " 'satu': 1,\n",
       " 'lintas': 1,\n",
       " 'ulama': 1,\n",
       " 'kh': 1,\n",
       " 'said': 1,\n",
       " 'aqil': 1,\n",
       " 'sirajd': 1,\n",
       " 'redam': 1,\n",
       " 'pimpin': 1,\n",
       " 'pusat': 1,\n",
       " 'surat': 1,\n",
       " 'nomor': 1,\n",
       " 'edr': 1,\n",
       " 'i': 1,\n",
       " 'e': 1,\n",
       " 'pandu': 1,\n",
       " 'siap': 1,\n",
       " 'iduladha': 1,\n",
       " 'h': 1,\n",
       " 'kondisi': 1,\n",
       " 'juli': 1,\n",
       " 'hukum': 1,\n",
       " 'tinggi': 1,\n",
       " 'poin': 1,\n",
       " 'sesuai': 2,\n",
       " 'instruksi': 2,\n",
       " 'no': 1,\n",
       " 'jokowi': 1,\n",
       " 'harap': 1,\n",
       " 'sejarah': 1,\n",
       " 'cacing': 1,\n",
       " 'duga': 1,\n",
       " 'proning': 1,\n",
       " 'miracle': 1,\n",
       " 'of': 1,\n",
       " 'sujud': 1,\n",
       " 'sembuh': 1,\n",
       " 'serang': 1}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zero_train_stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-67a2edc3d3d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'zero_train_stopwords.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mrs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'one'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'two'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'said'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Instantiate a dictionary, and for every word in the file,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'zero_train_stopwords.txt'"
     ]
    }
   ],
   "source": [
    "#SEMUA KATA (ALL)\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Read input file, note the encoding is specified here \n",
    "# It may be different in your text file\n",
    "file = open('tweet-output-count_train_positif.txt', encoding=\"utf8\")\n",
    "a= file.read()\n",
    "# Stopwords\n",
    "stopwords = set(line.strip() for line in open('zero_train_stopwords.txt'))\n",
    "stopwords = stopwords.union(set(['mr','mrs','one','two','said']))\n",
    "# Instantiate a dictionary, and for every word in the file, \n",
    "# Add to the dictionary if it doesn't exist. If it does, increase the count.\n",
    "wordcount = {}\n",
    "# To eliminate duplicates, remember to split by punctuation, and use case demiliters.\n",
    "for word in a.lower().split():\n",
    "    word = word.replace(\".\",\"\")\n",
    "    word = word.replace(\",\",\"\")\n",
    "    word = word.replace(\":\",\"\")\n",
    "    word = word.replace(\"\\\"\",\"\")\n",
    "    word = word.replace(\"!\",\"\")\n",
    "    word = word.replace(\"â€œ\",\"\")\n",
    "    word = word.replace(\"]\",\"\")\n",
    "    word = word.replace(\"[\",\"\")\n",
    "    word = word.replace(\"â€˜\",\"\")\n",
    "    word = word.replace(\"*\",\"\")\n",
    "    if word not in stopwords:\n",
    "        if word not in wordcount:\n",
    "            wordcount[word] = 1\n",
    "        else:\n",
    "            wordcount[word] += 1\n",
    "# Print most common word\n",
    "n_print = int(input(\"How many most common words to print: \"))\n",
    "print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\n",
    "word_counter = collections.Counter(wordcount)\n",
    "for word, count in word_counter.most_common(n_print):\n",
    "    print(word, \": \", count)\n",
    "# Close the file\n",
    "file.close()\n",
    "# Create a data frame of the most common words \n",
    "# Draw a bar chart\n",
    "lst = word_counter.most_common(n_print)\n",
    "df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "df.plot.bar(x='Word',y='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENGHITUNG IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_document = len(TWEET_DATA)\n",
    "\n",
    "def calc_IDF(__n_document, __DF):\n",
    "    IDF_Dict = {}\n",
    "    for term in __DF:\n",
    "        IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1))\n",
    "    return IDF_Dict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "IDF = calc_IDF(n_document, DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc TF-IDF\n",
    "def calc_TF_IDF(TF):\n",
    "    TF_IDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for key in TF:\n",
    "        TF_IDF_Dict[key] = TF[key] * IDF[key]\n",
    "    return TF_IDF_Dict\n",
    "\n",
    "#Stores the TF-IDF Series\n",
    "TWEET_DATA[\"TF-IDF_dict\"] = TWEET_DATA[\"TF_dict\"].apply(calc_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TF-IDF result\n",
    "index = 90\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\")\n",
    "for key in TWEET_DATA[\"TF-IDF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key] ,\"\\t\" , TWEET_DATA[\"TF-IDF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort descending by value for DF dictionary \n",
    "sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:50]\n",
    "\n",
    "# Create a list of unique words from sorted dictionay `sorted_DF`\n",
    "unique_term = [item[0] for item in sorted_DF]\n",
    "\n",
    "def calc_TF_IDF_Vec(__TF_IDF_Dict):\n",
    "    TF_IDF_vector = [0.0] * len(unique_term)\n",
    "\n",
    "    # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "    for i, term in enumerate(unique_term):\n",
    "        if term in __TF_IDF_Dict:\n",
    "            TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "    return TF_IDF_vector\n",
    "\n",
    "TWEET_DATA[\"TF_IDF_Vec\"] = TWEET_DATA[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec)\n",
    "\n",
    "print(\"print first row matrix TF_IDF_Vec Series\\n\")\n",
    "print(TWEET_DATA[\"TF_IDF_Vec\"][0])\n",
    "\n",
    "print(\"\\nmatrix size : \", len(TWEET_DATA[\"TF_IDF_Vec\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Series to List\n",
    "TF_IDF_Vec_List = np.array(TWEET_DATA[\"TF_IDF_Vec\"].to_list())\n",
    "\n",
    "# Sum element vector in axis=0 \n",
    "sums = TF_IDF_Vec_List.sum(axis=0)\n",
    "\n",
    "data = []\n",
    "\n",
    "for col, term in enumerate(unique_term):\n",
    "    data.append((term, sums[col]))\n",
    "    \n",
    "ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
